{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag Literature\n",
    "\n",
    "This Jupyter Notebook contains code and documentation for the Rag Literature project.\n",
    "To start, let's import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import shutil\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    texts = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                texts.append(text + \"\\n\")\n",
    "    return ''.join(texts)\n",
    "\n",
    "class SimpleDocument:\n",
    "    def __init__(self, page_content):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = {}\n",
    "\n",
    "def integrate_self_query_retriever(question, context):\n",
    "    enhanced_context = f\"Enhanced context based on self-querying logic: {context}\"\n",
    "    return enhanced_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prompt Setting\n",
    "\n",
    "Adjust the prompt according to the model you choose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ollama_llm(question, enhanced_context):\n",
    "    formatted_prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Context: {enhanced_context}\n",
    "    Omit information not present in the context.\n",
    "    Convert units for consistency. Separate entries with a clear delineation.\n",
    "    Each datablock seprated by enzyme-substrate ，organism and type .\n",
    "    Give the source of each data entry .\n",
    "    Extract and format information about all enzyme-substrate pair mentioned in the context, following this structure:\n",
    "    \n",
    "    Enzyme name: \n",
    "    EC number:( or'N/A')\n",
    "    Organism:( or'N/A')\n",
    "    Substrate:( or'N/A')\n",
    "    Type: (wild-type or mutant, specify mutation)\n",
    "    Protein Identifier: (UniProt Accession Number or NCBI ID)\n",
    "    Specific activity: (or Vmax)\n",
    "    KM Value: (in mM, or 'N/A')\n",
    "    Kcat Value: (per second, or 'N/A')\n",
    "    kcat/KM: (in mM⁻¹s⁻¹, or 'N/A')\n",
    "    pI Value: \n",
    "    pH Optimum:\n",
    "    Temperature Optimum: (in °C)\n",
    "    Molecular Weight: (in kDa)\n",
    "    Reaction pH: ( or'N/A')\n",
    "    Reaction Temperature: (in °C, or 'N/A')\n",
    "    Buffer Solution: ( or'N/A')\n",
    "    \n",
    "    \"\"\"\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_chain(question, pdf_text, pdf_path):\n",
    "    doc = SimpleDocument(pdf_text)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=500)\n",
    "    splits = text_splitter.split_documents([doc])\n",
    "    embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "    docs = vectorstore.similarity_search(question, k=4)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    enhanced_context = integrate_self_query_retriever(question, context)\n",
    "    return ollama_llm(question, enhanced_context)\n",
    "\n",
    "def process_folder(folder_path, question, output_file, log_file, problematic_folder):\n",
    "    if not os.path.exists(problematic_folder):\n",
    "        os.makedirs(problematic_folder)\n",
    "    \n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"r\") as f:\n",
    "            processed_files = f.read().splitlines()\n",
    "    else:\n",
    "        processed_files = []\n",
    "    \n",
    "    pdf_paths = [os.path.join(root, file) for root, dirs, files in os.walk(folder_path) for file in files if file.endswith(\".pdf\")]\n",
    "    \n",
    "    with tqdm(total=len(pdf_paths), unit=\"file\") as pbar:\n",
    "        for pdf_path in pdf_paths:\n",
    "            if pdf_path not in processed_files:\n",
    "                pdf_text = extract_text_from_pdf(pdf_path)\n",
    "                answer_text = rag_chain(question, pdf_text, pdf_path).strip()\n",
    "                # I use \"pmid\" to refer to the literature, but you can adjust it in the code\n",
    "                pmid = os.path.basename(pdf_path).split('_')[0]\n",
    "                with open(output_file, \"a\") as f:\n",
    "                    f.write(f\"PMID: {pmid}\\n{answer_text}\\n\\n\")\n",
    "        \n",
    "                with open(log_file, \"a\") as f:\n",
    "                    f.write(f\"{pdf_path}\\n\")\n",
    "                \n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model and folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama( model=\"llama2\")\n",
    "folder_path = \"/path/to/pdf/folder/\"\n",
    "question = \"Extract detailed information about all enzymes mentioned and experimental conditions from context correctly.\"\n",
    "output_file = \"output.txt\"\n",
    "log_file = \"processed_log.txt\"\n",
    "problematic_folder = \"problematic_files/\"\n",
    "process_folder(folder_path, question, output_file, log_file, problematic_folder)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
