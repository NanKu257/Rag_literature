{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed Attempt\n",
    "\n",
    "I have tried various methods in this notebook, but unfortunately, they did not work as expected. If you want to use this notebook, you may need to download additional packages such as pydantic, NLTK, and TensorRT, depending on your device requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pdfplumber\n",
    "import gc\n",
    "import unstructured\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from typing import Any, List\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "import uuid\n",
    "import logging\n",
    "from contextlib import closing\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import Runnable\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "class DictRunnable(Runnable):\n",
    "    def __init__(self, input_dict):\n",
    "        self.input_dict = input_dict\n",
    "\n",
    "    def invoke(self, **kwargs):\n",
    "        return self.input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Element(BaseModel):\n",
    "      type: str\n",
    "      content: Any\n",
    "\n",
    "def extract_elements_from_pdf(pdf_path):\n",
    "    raw_pdf_elements = partition_pdf(\n",
    "        filename=pdf_path,\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=1000,\n",
    "        new_after_n_chars=900,\n",
    "        combine_text_under_n_chars=50,\n",
    "        image_output_dir_path=\"image/\",\n",
    "    )\n",
    "\n",
    "    text_elements = []\n",
    "    table_elements = []\n",
    "    image_elements = []\n",
    "\n",
    "    for element in raw_pdf_elements:\n",
    "        if isinstance(element, unstructured.documents.elements.Text):\n",
    "            text_elements.append(Element(type=\"text\", content=str(element)))\n",
    "        elif isinstance(element, unstructured.documents.elements.Table):\n",
    "            table_elements.append(Element(type=\"table\", content=str(element)))\n",
    "        elif isinstance(element, unstructured.documents.elements.Image):\n",
    "            image_elements.append(Element(type=\"image\", content=element))\n",
    "    \n",
    "    return text_elements, table_elements, image_elements\n",
    "\n",
    "def summarize_element(element):\n",
    "    prompt_text = f\"\"\"\n",
    "    Be concise and contain all the information of {element.type}: {element.content}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text, template_format=\"jinja2\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | llm | StrOutputParser()\n",
    "    response = summarize_chain.invoke(element.content)  \n",
    "    return response\n",
    "    \n",
    "def build_retriever_with_summaries(text_elements, table_elements, image_elements , pdf_path):\n",
    "    id_key = \"doc_id\"\n",
    "    pdf_filename = os.path.basename(pdf_path)  \n",
    "    collection_name = pdf_filename.split(\"_\")[0] \n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=Chroma(collection_name=collection_name, embedding_function=OllamaEmbeddings(model='nomic-embed-text')),\n",
    "        docstore=InMemoryStore(),\n",
    "        id_key=id_key,\n",
    "    )\n",
    "    \n",
    "    text_summaries = [summarize_element(element) for element in text_elements]\n",
    "    text_ids = [str(uuid.uuid4()) for _ in text_elements]\n",
    "    summary_texts = [\n",
    "        Document(page_content=s, metadata={id_key: text_ids[i]})\n",
    "        for i, s in enumerate(text_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_texts)\n",
    "    retriever.docstore.mset(list(zip(text_ids, [element.content for element in text_elements])))\n",
    "\n",
    "    if table_elements:  # Add this line\n",
    "        table_summaries = [summarize_element(element) for element in table_elements]\n",
    "        table_ids = [str(uuid.uuid4()) for _ in table_elements]\n",
    "        summary_tables = [\n",
    "            Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "            for i, s in enumerate(table_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_tables)\n",
    "        retriever.docstore.mset(list(zip(table_ids, [element.content for element in table_elements])))\n",
    "    if image_elements:\n",
    "        image_summaries = [summarize_image(element) for element in image_elements]\n",
    "        image_ids = [str(uuid.uuid4()) for _ in image_elements]\n",
    "        summary_images = [\n",
    "        Document(page_content=s, metadata={id_key: image_ids[i]})\n",
    "        for i, s in enumerate(image_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_images)\n",
    "        retriever.docstore.mset(list(zip(image_ids, [element.content for element in image_elements])))\n",
    "    logging.info(\"检索器构建完成\")\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_file(pdf_path, question, output_file, log_file):\n",
    "    logging.info(f\"正在处理文件: {pdf_path}\")\n",
    "    text_elements, table_elements, image_elements = extract_elements_from_pdf(pdf_path)\n",
    "    logging.info(f\"已提取 {len(text_elements)} 个文本元素, {len(table_elements)} 个表格元素, 和 {len(image_elements)} 个图片元素\")\n",
    "    retriever = build_retriever_with_summaries(text_elements, table_elements, image_elements, pdf_path)\n",
    "    logging.info(\"已构建带有摘要的检索器\")\n",
    "    \n",
    "    response = ollama_llm(question, retriever)\n",
    "    \n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(response + \"\\n\\n\")\n",
    "        \n",
    "    logging.info(\"文件处理完成\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"{pdf_path}\\n\")\n",
    "    gc.collect()\n",
    "\n",
    "def process_folder(folder_path, question, output_file, log_file):\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"r\") as f:\n",
    "            processed_files = f.read().splitlines()\n",
    "    else:\n",
    "        processed_files = []\n",
    "\n",
    "    pdf_paths = [os.path.join(root, file) for root, dirs, files in os.walk(folder_path) for file in files if file.endswith(\".pdf\")]\n",
    "    pbar = tqdm(pdf_paths, desc=\"总体进度\")\n",
    "\n",
    "    for pdf_path in pbar:\n",
    "        if pdf_path not in processed_files:\n",
    "            process_file(pdf_path, question, output_file, log_file)\n",
    "        pbar.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_llm(question, retriever):\n",
    "    formatted_prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "\n",
    "    Omit information not present in the context.\n",
    "    Convert units for consistency. Separate entries with a clear delineation.\n",
    "    Each datablock seprated by enzyme-substrate ，organism and type .\n",
    "    Give the source of each data entry .\n",
    "    Extract and format information about all enzyme-substrate pair mentioned in the context, following this structure:\n",
    "    \n",
    "    Enzyme name: \n",
    "    EC number:( or'N/A')\n",
    "    Organism:( or'N/A')\n",
    "    Substrate:( or'N/A')\n",
    "    Type: (wild-type or mutant, specify mutation)\n",
    "    Protein Identifier: (UniProt Accession Number or NCBI ID)\n",
    "    Specific activity: (or Vmax)\n",
    "    KM Value: (in mM, or 'N/A')\n",
    "    Kcat Value: (per second, or 'N/A')\n",
    "    kcat/KM: (in mM⁻¹s⁻¹, or 'N/A')\n",
    "    pI Value: \n",
    "    pH Optimum:\n",
    "    Temperature Optimum: (in °C)\n",
    "    Molecular Weight: (in kDa)\n",
    "    Reaction pH: ( or'N/A')\n",
    "    Reaction Temperature: (in °C, or 'N/A')\n",
    "    Buffer Solution: ( or'N/A')\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=formatted_prompt, input_variables=[\"question\", \"context\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    relevant_docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join(relevant_docs) \n",
    "    return chain.run(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path =\"/path/to/pdf/folder/\"\n",
    "question = \"Extract detailed information about all enzymes mentioned and experimental conditions from {context} correctly. \"\n",
    "output_file = \"output.txt\"\n",
    "log_file = \"processed_files.log\"\n",
    "process_folder(folder_path, question, output_file, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After running the code, you should be able to see the output.txt file in the specified folder.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
